{"cells":[{"cell_type":"markdown","id":"6fad6f53","metadata":{},"source":["# Question 1"]},{"cell_type":"markdown","id":"f92a6510","metadata":{},"source":["# Spark RDD"]},{"cell_type":"markdown","id":"18c4eab6","metadata":{},"source":["We continue to analyse the DBLP dataset available in the file author-large.txt. This time, we want to find the top 10 pairs of authors who published the largest number of papers together (with possible other collaborators). For example, if authors a, b and c published a paper with title t, then this contributes one joint publication for each author pair (a,b), (a,c) and (b,c). Use the first column of the input data for the author names and use the third column of the input data for the publication title."]},{"cell_type":"code","execution_count":null,"id":"45d218d6","metadata":{},"outputs":[],"source":["#plan: 1. transfrom a row string to a list of string, remain only author name and the paper they published\n","#      2. group author by the pair of paper they published\n","#      3.remove paper name row\n","#      4. find out different pair of co-author\n","#      5. count the number of paper each pair co-author\n","#      6. list the top 10 pair with author name and the number of publication\n"]},{"cell_type":"code","execution_count":3,"id":"39da13a5","metadata":{},"outputs":[],"source":["#load data \n","authorfile = sc.textFile( \"gs://st446-w3/author-large.txt\")"]},{"cell_type":"code","execution_count":4,"id":"219c25da","metadata":{},"outputs":[],"source":["# 1. transfrom a row string to a list of string (tokenization)\n","def get_author_paper(line):\n","    import re\n","    import numpy as np\n","    \n","    row = line.strip()\n","    words = np.array(row.split(\"\\t\")) \n","    return (words)\n","\n","# 2. Remain only author name and paper\n","counts = authorfile.map(lambda line: get_author_paper(line))\n","counts = counts.map(lambda row: (row[2], row[0]))"]},{"cell_type":"code","execution_count":5,"id":"f8a42be9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'Jurgen Annevelink'),\n"," ('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'Rafiul Ahad'),\n"," ('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'Amelia Carlson'),\n"," ('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'Daniel H. Fishman'),\n"," ('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'Michael L. Heytens'),\n"," ('Object SQL - A Language for the Design and Implementation of Object Databases.',\n","  'William Kent'),\n"," ('OQL[C++]: Extending C++ with an Object Query Capability.',\n","  'Jos A. Blakeley'),\n"," ('Transaction Management in Multidatabase Systems.', 'Yuri Breitbart'),\n"," ('Transaction Management in Multidatabase Systems.', 'Hector Garcia-Molina'),\n"," ('Transaction Management in Multidatabase Systems.', 'Abraham Silberschatz')]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#view the first 10 rows\n","counts.take(10)"]},{"cell_type":"code","execution_count":6,"id":"ba329539","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[('OQL[C++]: Extending C++ with an Object Query Capability.',\n","  ['Jos A. Blakeley']),\n"," ('Active Database Systems.',\n","  ['Umeshwar Dayal', 'Eric N. Hanson', 'Jennifer Widom']),\n"," ('Where Object-Oriented DBMSs Should Do Better: A Critique Based on Early Experiences.',\n","  ['Angelika Kotz Dittrich', 'Klaus R. Dittrich']),\n"," ('An Object-Oriented DBMS War Story: Developing a Genome Mapping Database in C++.',\n","  ['Nathan Goodman']),\n"," ('Cooperative Transactions for Multiuser Environments.', ['Gail E. Kaiser']),\n"," ('Schema Architecture of the UniSQL/M Multidatabase System',\n","  ['William Kelley',\n","   'Sunit K. Gala',\n","   'Won Kim',\n","   'Tom C. Reyes',\n","   'Bruce Graham']),\n"," ('Physical Object Management.', ['Alfons Kemper', 'Guido Moerkotte']),\n"," ('Introduction to Part 1: Next-Generation Database Technology.', ['Won Kim']),\n"," ('Introduction to Part 2: Technology for Interoperating Legacy Databases.',\n","  ['Won Kim']),\n"," ('The POSC Solution to Managing E&P Data.', ['Vincent J. Kowalski'])]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#group author by paper they publish\n","xx=counts.groupByKey().mapValues(list)\n","xx.take(10)"]},{"cell_type":"code","execution_count":7,"id":"1bec4f2e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["810021"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#check the number of unique key/ publishment\n","distinct_paper = xx.map(lambda row: row[0]).distinct()\n","distinct_paper.count()"]},{"cell_type":"code","execution_count":8,"id":"8707e560","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[['Jos A. Blakeley'],\n"," ['Umeshwar Dayal', 'Eric N. Hanson', 'Jennifer Widom'],\n"," ['Angelika Kotz Dittrich', 'Klaus R. Dittrich'],\n"," ['Nathan Goodman'],\n"," ['Gail E. Kaiser'],\n"," ['William Kelley',\n","  'Sunit K. Gala',\n","  'Won Kim',\n","  'Tom C. Reyes',\n","  'Bruce Graham'],\n"," ['Alfons Kemper', 'Guido Moerkotte'],\n"," ['Won Kim'],\n"," ['Won Kim'],\n"," ['Vincent J. Kowalski']]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#remove paper name, remain only author\n","xxx = xx.map(lambda row: (row[1]))\n","xxx.take(10)"]},{"cell_type":"code","execution_count":36,"id":"396fa6e2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/02/24 16:34:47 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000002 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:34:47.155]Container killed on request. Exit code is 143\n","[2022-02-24 16:34:47.157]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:34:47.167]Killed by external signal\n",".\n","22/02/24 16:34:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1645704662416_0001_01_000002 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:34:47.155]Container killed on request. Exit code is 143\n","[2022-02-24 16:34:47.157]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:34:47.167]Killed by external signal\n",".\n","22/02/24 16:34:47 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000002 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:34:47.155]Container killed on request. Exit code is 143\n","[2022-02-24 16:34:47.157]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:34:47.167]Killed by external signal\n",".\n","22/02/24 16:34:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 17.0 (TID 17) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000002 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:34:47.155]Container killed on request. Exit code is 143\n","[2022-02-24 16:34:47.157]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:34:47.167]Killed by external signal\n",".\n","22/02/24 16:34:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 17.0 (TID 18) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000002 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:34:47.155]Container killed on request. Exit code is 143\n","[2022-02-24 16:34:47.157]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:34:47.167]Killed by external signal\n",".\n","22/02/24 16:35:02 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000003 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:35:01.959]Container killed on request. Exit code is 143\n","[2022-02-24 16:35:01.959]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:35:01.959]Killed by external signal\n",".\n","22/02/24 16:35:02 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1645704662416_0001_01_000003 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:35:01.959]Container killed on request. Exit code is 143\n","[2022-02-24 16:35:01.959]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:35:01.959]Killed by external signal\n",".\n","22/02/24 16:35:02 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 3 on st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000003 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:35:01.959]Container killed on request. Exit code is 143\n","[2022-02-24 16:35:01.959]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:35:01.959]Killed by external signal\n",".\n","22/02/24 16:35:02 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 17.0 (TID 20) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000003 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:35:01.959]Container killed on request. Exit code is 143\n","[2022-02-24 16:35:01.959]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:35:01.959]Killed by external signal\n",".\n","22/02/24 16:35:02 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 17.0 (TID 19) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000003 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:35:01.959]Container killed on request. Exit code is 143\n","[2022-02-24 16:35:01.959]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:35:01.959]Killed by external signal\n",".\n","22/02/24 16:41:12 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000004 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 137. Diagnostics: [2022-02-24 16:41:12.190]Container killed on request. Exit code is 137\n","[2022-02-24 16:41:12.191]Container exited with a non-zero exit code 137. \n","[2022-02-24 16:41:12.192]Killed by external signal\n",".\n","22/02/24 16:41:12 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1645704662416_0001_01_000004 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 137. Diagnostics: [2022-02-24 16:41:12.190]Container killed on request. Exit code is 137\n","[2022-02-24 16:41:12.191]Container exited with a non-zero exit code 137. \n","[2022-02-24 16:41:12.192]Killed by external signal\n",".\n","22/02/24 16:41:12 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000004 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 137. Diagnostics: [2022-02-24 16:41:12.190]Container killed on request. Exit code is 137\n","[2022-02-24 16:41:12.191]Container exited with a non-zero exit code 137. \n","[2022-02-24 16:41:12.192]Killed by external signal\n",".\n","22/02/24 16:41:12 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.1 in stage 17.0 (TID 22) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000004 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 137. Diagnostics: [2022-02-24 16:41:12.190]Container killed on request. Exit code is 137\n","[2022-02-24 16:41:12.191]Container exited with a non-zero exit code 137. \n","[2022-02-24 16:41:12.192]Killed by external signal\n",".\n","22/02/24 16:41:12 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.1 in stage 17.0 (TID 21) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000004 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 137. Diagnostics: [2022-02-24 16:41:12.190]Container killed on request. Exit code is 137\n","[2022-02-24 16:41:12.191]Container exited with a non-zero exit code 137. \n","[2022-02-24 16:41:12.192]Killed by external signal\n",".\n","22/02/24 16:42:25 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000005 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:42:25.262]Container killed on request. Exit code is 143\n","[2022-02-24 16:42:25.262]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:42:25.262]Killed by external signal\n",".\n","22/02/24 16:42:25 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1645704662416_0001_01_000005 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:42:25.262]Container killed on request. Exit code is 143\n","[2022-02-24 16:42:25.262]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:42:25.262]Killed by external signal\n",".\n","22/02/24 16:42:25 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 5 on st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000005 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:42:25.262]Container killed on request. Exit code is 143\n","[2022-02-24 16:42:25.262]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:42:25.262]Killed by external signal\n",".\n","22/02/24 16:42:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.1 in stage 17.0 (TID 23) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000005 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:42:25.262]Container killed on request. Exit code is 143\n","[2022-02-24 16:42:25.262]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:42:25.262]Killed by external signal\n",".\n","22/02/24 16:42:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.1 in stage 17.0 (TID 24) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000005 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:42:25.262]Container killed on request. Exit code is 143\n","[2022-02-24 16:42:25.262]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:42:25.262]Killed by external signal\n",".\n","22/02/24 16:45:47 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000006 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:45:47.259]Container killed on request. Exit code is 143\n","[2022-02-24 16:45:47.260]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:45:47.260]Killed by external signal\n",".\n","22/02/24 16:45:47 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1645704662416_0001_01_000006 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:45:47.259]Container killed on request. Exit code is 143\n","[2022-02-24 16:45:47.260]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:45:47.260]Killed by external signal\n",".\n","22/02/24 16:45:47 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 6 on st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000006 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:45:47.259]Container killed on request. Exit code is 143\n","[2022-02-24 16:45:47.260]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:45:47.260]Killed by external signal\n",".\n","22/02/24 16:45:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.2 in stage 17.0 (TID 26) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000006 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:45:47.259]Container killed on request. Exit code is 143\n","[2022-02-24 16:45:47.260]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:45:47.260]Killed by external signal\n",".\n","22/02/24 16:45:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.2 in stage 17.0 (TID 25) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000006 on host: st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:45:47.259]Container killed on request. Exit code is 143\n","[2022-02-24 16:45:47.260]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:45:47.260]Killed by external signal\n",".\n","22/02/24 16:48:33 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000007 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:48:33.059]Container killed on request. Exit code is 143\n","[2022-02-24 16:48:33.059]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:48:33.060]Killed by external signal\n",".\n","22/02/24 16:48:33 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1645704662416_0001_01_000007 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:48:33.059]Container killed on request. Exit code is 143\n","[2022-02-24 16:48:33.059]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:48:33.060]Killed by external signal\n",".\n","22/02/24 16:48:33 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 7 on st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000007 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:48:33.059]Container killed on request. Exit code is 143\n","[2022-02-24 16:48:33.059]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:48:33.060]Killed by external signal\n",".\n","22/02/24 16:48:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.2 in stage 17.0 (TID 28) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000007 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:48:33.059]Container killed on request. Exit code is 143\n","[2022-02-24 16:48:33.059]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:48:33.060]Killed by external signal\n",".\n","22/02/24 16:48:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.2 in stage 17.0 (TID 27) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000007 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:48:33.059]Container killed on request. Exit code is 143\n","[2022-02-24 16:48:33.059]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:48:33.060]Killed by external signal\n",".\n","22/02/24 16:51:42 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n","[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:51:41.827]Killed by external signal\n",".\n","22/02/24 16:51:42 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 8 for reason Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n","[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:51:41.827]Killed by external signal\n",".\n","22/02/24 16:51:42 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 8 on st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n","[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:51:41.827]Killed by external signal\n",".\n","22/02/24 16:51:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.3 in stage 17.0 (TID 29) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n","[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:51:41.827]Killed by external signal\n",".\n","22/02/24 16:51:42 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 17.0 failed 4 times; aborting job\n","22/02/24 16:51:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.3 in stage 17.0 (TID 30) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n","[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n","[2022-02-24 16:51:41.827]Killed by external signal\n",".\n","22/02/24 16:51:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.3 in stage 17.0 (TID 31) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 9): TaskKilled (Stage cancelled)\n","22/02/24 16:51:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.3 in stage 17.0 (TID 32) (st446-cluster-w-0.europe-west2-b.c.distributed-computing-340216.internal executor 9): TaskKilled (Stage cancelled)\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 17.0 failed 4 times, most recent failure: Lost task 1.3 in stage 17.0 (TID 29) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n[2022-02-24 16:51:41.827]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","Input \u001B[0;32mIn [36]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28msorted\u001B[39m(\u001B[43mxxx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcartesian\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxxx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:949\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    940\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    941\u001B[0m \u001B[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001B[39;00m\n\u001B[1;32m    942\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001B[39;00m\n\u001B[1;32m    947\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext) \u001B[38;5;28;01mas\u001B[39;00m css:\n\u001B[0;32m--> 949\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1308\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 17.0 failed 4 times, most recent failure: Lost task 1.3 in stage 17.0 (TID 29) (st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1645704662416_0001_01_000008 on host: st446-cluster-w-1.europe-west2-b.c.distributed-computing-340216.internal. Exit status: 143. Diagnostics: [2022-02-24 16:51:41.827]Container killed on request. Exit code is 143\n[2022-02-24 16:51:41.827]Container exited with a non-zero exit code 143. \n[2022-02-24 16:51:41.827]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"]}],"source":["sorted(xxx.cartesian(xxx).collect())"]},{"cell_type":"code","execution_count":13,"id":"7ef3cd13","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 6:============================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["Wen Gao: 370\n","Philip S. Yu: 370\n","Thomas S. Huang: 368\n","Edwin R. Hancock: 353\n","Mahmut T. Kandemir: 346\n","Mario Piattini: 320\n","Wei Li: 319\n","Elisa Bertino: 319\n","Sudhakar M. Reddy: 314\n","Alberto L. Sangiovanni-Vincentelli: 309\n","Wei Wang: 295\n","Jiawei Han: 289\n","Makoto Takizawa: 273\n","Irith Pomeranz: 272\n","Wei Zhang: 270\n","Lei Zhang: 269\n","Ming Li: 268\n","Li Zhang: 267\n","Hai Jin: 262\n","Hector Garcia-Molina: 258\n","Moshe Y. Vardi: 257\n","Hans-Peter Kriegel: 255\n","Hans-Peter Seidel: 251\n","Kaushik Roy: 251\n","Francky Catthoor: 248\n","Tharam S. Dillon: 245\n","Moti Yung: 242\n","Katsumi Tanaka: 235\n","Qing Li: 235\n","Yan Zhang: 233\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# show the top 10 pairs of authors that co-authors the most number of papers\n","output = counts.takeOrdered(10, key = lambda x: -x[1])\n","for word, count in output:\n","    print (word + ': ' + str(count))"]},{"cell_type":"markdown","id":"311a4059","metadata":{},"source":["# Question 2"]},{"cell_type":"markdown","id":"7fff172d","metadata":{},"source":["# Spark SQL"]},{"cell_type":"markdown","id":"0d157ee2","metadata":{},"source":["Do the same as in problem P1, but this time use the Spark SQL API (covered in Week 4). You may find useful to consult the Spark SQL documentation as well."]},{"cell_type":"code","execution_count":22,"id":"0d5cbe8c","metadata":{},"outputs":[],"source":["#load data\n","filename = 'gs://st446-w3/author-large.txt'\n","\n","from pyspark.sql.types import *\n","\n","schema = StructType([\n","    StructField(\"author\", StringType(), True),    \n","    StructField(\"journal\", StringType(), True),\n","    StructField(\"title\", StringType(), True),\n","    StructField(\"year\", LongType(), True)\n","])\n","\n","author_large = spark.read.csv(filename, header='false', schema=schema, sep='\\t')\n","author_large.createOrReplaceTempView(\"author_large\")"]},{"cell_type":"code","execution_count":25,"id":"9f74f33b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- author: string (nullable = true)\n"," |-- journal: string (nullable = true)\n"," |-- title: string (nullable = true)\n"," |-- year: long (nullable = true)\n","\n","+--------------------+--------------------+--------------------+----+\n","|              author|             journal|               title|year|\n","+--------------------+--------------------+--------------------+----+\n","|   Jurgen Annevelink|Modern Database S...|Object SQL - A La...|1995|\n","|         Rafiul Ahad|Modern Database S...|Object SQL - A La...|1995|\n","|      Amelia Carlson|Modern Database S...|Object SQL - A La...|1995|\n","|   Daniel H. Fishman|Modern Database S...|Object SQL - A La...|1995|\n","|  Michael L. Heytens|Modern Database S...|Object SQL - A La...|1995|\n","|        William Kent|Modern Database S...|Object SQL - A La...|1995|\n","|     Jos A. Blakeley|Modern Database S...|OQL[C++]: Extendi...|1995|\n","|      Yuri Breitbart|Modern Database S...|Transaction Manag...|1995|\n","|Hector Garcia-Molina|Modern Database S...|Transaction Manag...|1995|\n","|Abraham Silberschatz|Modern Database S...|Transaction Manag...|1995|\n","+--------------------+--------------------+--------------------+----+\n","only showing top 10 rows\n","\n"]}],"source":["#check the schema, and first 10 row\n","author_large.printSchema()\n","author_large.show(10)"]},{"cell_type":"code","execution_count":35,"id":"16a7ac56","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|              author|               title|\n","+--------------------+--------------------+\n","|   Jurgen Annevelink|Object SQL - A La...|\n","|         Rafiul Ahad|Object SQL - A La...|\n","|      Amelia Carlson|Object SQL - A La...|\n","|   Daniel H. Fishman|Object SQL - A La...|\n","|  Michael L. Heytens|Object SQL - A La...|\n","|        William Kent|Object SQL - A La...|\n","|     Jos A. Blakeley|OQL[C++]: Extendi...|\n","|      Yuri Breitbart|Transaction Manag...|\n","|Hector Garcia-Molina|Transaction Manag...|\n","|Abraham Silberschatz|Transaction Manag...|\n","+--------------------+--------------------+\n","only showing top 10 rows\n","\n"]}],"source":["#drop the journal and year column\n","author_large1= author_large.drop(\"journal\",\"year\")\n","author_large1.show(10)"]},{"cell_type":"code","execution_count":null,"id":"9a59a108","metadata":{},"outputs":[],"source":["#next step pair author based on the paper they publish\n","#count the number of same pair,and rank them."]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}